{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이전 코드에서 생성한 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# continual learning을 위한 데이터셋\n",
    "\n",
    "train_data_level3 = {\"A\":[0,0],\"B\":[0,0],\"C\":[0,0]}\n",
    "train_data_level3[\"A\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_train_data_A.npy\")\n",
    "train_data_level3[\"B\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_train_data_B.npy\")\n",
    "train_data_level3[\"C\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_train_data_C.npy\")\n",
    "\n",
    "train_data_level3[\"A\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_train_data_A.npy\")\n",
    "train_data_level3[\"B\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_train_data_B.npy\")\n",
    "train_data_level3[\"C\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_train_data_C.npy\")\n",
    "\n",
    "val_data_level3 = {\"A\":[0,0],\"B\":[0,0],\"C\":[0,0]}\n",
    "val_data_level3[\"A\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_val_data_A.npy\")\n",
    "val_data_level3[\"B\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_val_data_B.npy\")\n",
    "val_data_level3[\"C\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_val_data_C.npy\")\n",
    "\n",
    "val_data_level3[\"A\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_val_data_A.npy\")\n",
    "val_data_level3[\"B\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_val_data_B.npy\")\n",
    "val_data_level3[\"C\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_val_data_C.npy\")\n",
    "\n",
    "test_data_level3 = {\"A\":[0,0],\"B\":[0,0],\"C\":[0,0]}\n",
    "test_data_level3[\"A\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_test_data_A.npy\")\n",
    "test_data_level3[\"B\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_test_data_B.npy\")\n",
    "test_data_level3[\"C\"][0] = np.load(\"../dataset/continual_learning_dataset/X_continual_test_data_C.npy\")\n",
    "\n",
    "test_data_level3[\"A\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_test_data_A.npy\")\n",
    "test_data_level3[\"B\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_test_data_B.npy\")\n",
    "test_data_level3[\"C\"][1] = np.load(\"../dataset/continual_learning_dataset/y_continual_test_data_C.npy\")\n",
    "\n",
    "\n",
    "# joint training 을 위한 데이터셋\n",
    "\n",
    "X_train_3 = np.load(\"../dataset/joint_training_dataset/X_joint_train_data.npy\")\n",
    "y_train_3 = np.load(\"../dataset/joint_training_dataset/y_joint_train_data.npy\")\n",
    "X_val_3 = np.load(\"../dataset/joint_training_dataset/X_joint_val_data.npy\")\n",
    "y_val_3 = np.load(\"../dataset/joint_training_dataset/y_joint_val_data.npy\")\n",
    "X_test_3 = np.load(\"../dataset/joint_training_dataset/X_joint_test_data.npy\")\n",
    "y_test_3 = np.load(\"../dataset/joint_training_dataset/y_joint_test_data.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.208034 ,  0.5018176,  0.1888077, ..., -0.9396821,  0.4002334,\n",
       "       -0.5487765])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_level3[\"B\"][0][753]                 # 756,4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00088406, -0.0018706 ,  0.00108903, ..., -0.00154175,\n",
       "        -0.00351483, -0.00581676],\n",
       "       [ 0.00470634,  0.00404865,  0.00306211, ...,  0.00898136,\n",
       "         0.0099679 ,  0.00963905],\n",
       "       [ 0.0043775 ,  0.00536404,  0.0043775 , ...,  0.00602173,\n",
       "         0.00635058,  0.00536404],\n",
       "       ...,\n",
       "       [ 0.00273326,  0.00569289,  0.00799482, ...,  0.00273326,\n",
       "         0.00141787,  0.00076018],\n",
       "       [ 0.01029675,  0.00832366,  0.00766597, ...,  0.01818908,\n",
       "         0.01522945,  0.01226983],\n",
       "       [ 0.00470634,  0.00536404,  0.00733712, ...,  0.00043133,\n",
       "         0.00207557,  0.00207557]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_level3[\"C\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터로더 래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 디렉토리로 작업환경 이동\n",
    "import os\n",
    "os.chdir(os.path.join(globals()['_dh'][0], \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfb.dataset import *\n",
    "from dfb.processing import *\n",
    "import experiment\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "model_name = \"wdcnn2\"\n",
    "\n",
    "sample_length = experiment.get_sample_length(model_name)\n",
    "tf_data = experiment.get_transform(model_name)\n",
    "tf_data = transforms.Compose(tf_data)\n",
    "tf_label = NpToTensor()\n",
    "batch_size = 128\n",
    "num_worker = 4\n",
    "\n",
    "data_handler = DatasetHandler()\n",
    "\n",
    "data_handler.assign(\n",
    "    X_train_3, y_train_3, X_val_3, y_val_3, X_test_3, y_test_3,\n",
    "    sample_length, \"3\", tf_data, tf_label, batch_size, num_worker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 수정되는 경우 해당 셀에서 바로 갱신되도록 autoreload사용\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dfb.model.wdcnn2 import *\n",
    "from dfb.model.wdcnn3 import *\n",
    "from dfb.model.wdcnn4 import *\n",
    "# 클래스 수는 맘대로 지정\n",
    "\n",
    "# 클래스 수 10개짜리 모델 만들기\n",
    "model = WDCNN2(n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/data/home/happy113200/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /data/home/happy113200/DeepFault/logs/test/best_model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/data/home/happy113200/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from dfb.trainmodule import *\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# ticnn 플래그는 일단 신경쓸 필요 X\n",
    "training_module = PlModule(model, optimizer, loss, True)\n",
    "\n",
    "n_steps_d = len(data_handler.dataloaders[\"3\"][\"train\"].dataset) // (batch_size)\n",
    "\n",
    "# val_loss 가 가장 작은 모델을 저장하는 콜백\n",
    "callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                        dirpath=f\"./logs/test/best_model\",\n",
    "                                        filename=f\"model\",\n",
    "                                        save_top_k=1,\n",
    "                                        mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    #gpus = [0],\n",
    "    max_epochs=100,\n",
    "    val_check_interval= n_steps_d,\n",
    "    default_root_dir=\"./logs/test\",\n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "result = trainer.fit(model=training_module,\n",
    "                     train_dataloaders=data_handler.dataloaders[\"3\"][\"train\"],\n",
    "                     val_dataloaders=data_handler.dataloaders[\"3\"][\"val\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 106.00it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9853723645210266\n",
      "        test_loss          0.054738495498895645\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "training_module.load_from_checkpoint(f\"./logs/test/best_model/model-v36.ckpt\",\n",
    "                                            model=model, optimizer=optimizer,\n",
    "                                            loss_fn=loss)\n",
    "result = trainer.test(model=training_module, dataloaders=data_handler.dataloaders[\"3\"][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from dfb.trainmodule import PlModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Adam' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aaa \u001b[39m=\u001b[39m training_module\u001b[39m.\u001b[39;49mload_from_checkpoint(\u001b[39m\"\u001b[39;49m\u001b[39m./logs/test/best_model/model-v35.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, optimizer, loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:137\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_from_checkpoint(\n\u001b[1;32m    138\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m    139\u001b[0m         checkpoint_path,\n\u001b[1;32m    140\u001b[0m         map_location,\n\u001b[1;32m    141\u001b[0m         hparams_file,\n\u001b[1;32m    142\u001b[0m         strict,\n\u001b[1;32m    143\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m     map_location \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m storage, loc: storage\n\u001b[1;32m    183\u001b[0m \u001b[39mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m--> 184\u001b[0m     checkpoint \u001b[39m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m hparams_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     extension \u001b[39m=\u001b[39m hparams_file\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/utilities/cloud_io.py:47\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     45\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m     46\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39mopen(path_or_url, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(f, map_location\u001b[39m=\u001b[39;49mmap_location)\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/torch/serialization.py:976\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m--> 976\u001b[0m     result \u001b[39m=\u001b[39m map_location(storage, location)\n\u001b[1;32m    977\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m         result \u001b[39m=\u001b[39m default_restore_location(storage, location)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Adam' object is not callable"
     ]
    }
   ],
   "source": [
    "aaa = training_module.load_from_checkpoint(\"./logs/test/best_model/model-v35.ckpt\", optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for WDCNN2:\n\tMissing key(s) in state_dict: \"conv_layers.0.weight\", \"conv_layers.0.bias\", \"conv_layers.1.weight\", \"conv_layers.1.bias\", \"conv_layers.1.running_mean\", \"conv_layers.1.running_var\", \"conv_layers.4.weight\", \"conv_layers.4.bias\", \"conv_layers.5.weight\", \"conv_layers.5.bias\", \"conv_layers.5.running_mean\", \"conv_layers.5.running_var\", \"conv_layers.8.weight\", \"conv_layers.8.bias\", \"conv_layers.9.weight\", \"conv_layers.9.bias\", \"conv_layers.9.running_mean\", \"conv_layers.9.running_var\", \"conv_layers.12.weight\", \"conv_layers.12.bias\", \"conv_layers.13.weight\", \"conv_layers.13.bias\", \"conv_layers.13.running_mean\", \"conv_layers.13.running_var\", \"linear_layers.0.weight\", \"linear_layers.0.bias\", \"linear_layers.1.weight\", \"linear_layers.1.bias\", \"linear_layers.1.running_mean\", \"linear_layers.1.running_var\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"model.conv_layers.0.weight\", \"model.conv_layers.0.bias\", \"model.conv_layers.1.weight\", \"model.conv_layers.1.bias\", \"model.conv_layers.1.running_mean\", \"model.conv_layers.1.running_var\", \"model.conv_layers.1.num_batches_tracked\", \"model.conv_layers.4.weight\", \"model.conv_layers.4.bias\", \"model.conv_layers.5.weight\", \"model.conv_layers.5.bias\", \"model.conv_layers.5.running_mean\", \"model.conv_layers.5.running_var\", \"model.conv_layers.5.num_batches_tracked\", \"model.conv_layers.8.weight\", \"model.conv_layers.8.bias\", \"model.conv_layers.9.weight\", \"model.conv_layers.9.bias\", \"model.conv_layers.9.running_mean\", \"model.conv_layers.9.running_var\", \"model.conv_layers.9.num_batches_tracked\", \"model.conv_layers.12.weight\", \"model.conv_layers.12.bias\", \"model.conv_layers.13.weight\", \"model.conv_layers.13.bias\", \"model.conv_layers.13.running_mean\", \"model.conv_layers.13.running_var\", \"model.conv_layers.13.num_batches_tracked\", \"model.linear_layers.0.weight\", \"model.linear_layers.0.bias\", \"model.linear_layers.1.weight\", \"model.linear_layers.1.bias\", \"model.linear_layers.1.running_mean\", \"model.linear_layers.1.running_var\", \"model.linear_layers.1.num_batches_tracked\", \"model.head.weight\", \"model.head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# 2. 모델 초기화 및 훈련된 가중치 불러오기\u001b[39;00m\n\u001b[1;32m      8\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_checkpoint_path)\n\u001b[0;32m----> 9\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m'\u001b[39;49m])  \u001b[39m# 훈련된 가중치 불러오기\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for WDCNN2:\n\tMissing key(s) in state_dict: \"conv_layers.0.weight\", \"conv_layers.0.bias\", \"conv_layers.1.weight\", \"conv_layers.1.bias\", \"conv_layers.1.running_mean\", \"conv_layers.1.running_var\", \"conv_layers.4.weight\", \"conv_layers.4.bias\", \"conv_layers.5.weight\", \"conv_layers.5.bias\", \"conv_layers.5.running_mean\", \"conv_layers.5.running_var\", \"conv_layers.8.weight\", \"conv_layers.8.bias\", \"conv_layers.9.weight\", \"conv_layers.9.bias\", \"conv_layers.9.running_mean\", \"conv_layers.9.running_var\", \"conv_layers.12.weight\", \"conv_layers.12.bias\", \"conv_layers.13.weight\", \"conv_layers.13.bias\", \"conv_layers.13.running_mean\", \"conv_layers.13.running_var\", \"linear_layers.0.weight\", \"linear_layers.0.bias\", \"linear_layers.1.weight\", \"linear_layers.1.bias\", \"linear_layers.1.running_mean\", \"linear_layers.1.running_var\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"model.conv_layers.0.weight\", \"model.conv_layers.0.bias\", \"model.conv_layers.1.weight\", \"model.conv_layers.1.bias\", \"model.conv_layers.1.running_mean\", \"model.conv_layers.1.running_var\", \"model.conv_layers.1.num_batches_tracked\", \"model.conv_layers.4.weight\", \"model.conv_layers.4.bias\", \"model.conv_layers.5.weight\", \"model.conv_layers.5.bias\", \"model.conv_layers.5.running_mean\", \"model.conv_layers.5.running_var\", \"model.conv_layers.5.num_batches_tracked\", \"model.conv_layers.8.weight\", \"model.conv_layers.8.bias\", \"model.conv_layers.9.weight\", \"model.conv_layers.9.bias\", \"model.conv_layers.9.running_mean\", \"model.conv_layers.9.running_var\", \"model.conv_layers.9.num_batches_tracked\", \"model.conv_layers.12.weight\", \"model.conv_layers.12.bias\", \"model.conv_layers.13.weight\", \"model.conv_layers.13.bias\", \"model.conv_layers.13.running_mean\", \"model.conv_layers.13.running_var\", \"model.conv_layers.13.num_batches_tracked\", \"model.linear_layers.0.weight\", \"model.linear_layers.0.bias\", \"model.linear_layers.1.weight\", \"model.linear_layers.1.bias\", \"model.linear_layers.1.running_mean\", \"model.linear_layers.1.running_var\", \"model.linear_layers.1.num_batches_tracked\", \"model.head.weight\", \"model.head.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dfb.trainmodule import *  # 모듈 및 클래스 임포트\n",
    "\n",
    "# 1. 훈련된 모델의 체크포인트 파일 경로\n",
    "model_checkpoint_path = \"./logs/test/best_model/model-v36.ckpt\"  # 체크포인트 파일 경로\n",
    "\n",
    "# 2. 모델 초기화 및 훈련된 가중치 불러오기\n",
    "checkpoint = torch.load(model_checkpoint_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])  # 훈련된 가중치 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
