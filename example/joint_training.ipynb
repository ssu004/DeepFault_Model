{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 훈련 기초 (joint training)\n",
    "\n",
    "## 데이터셋 다운로드\n",
    "\n",
    "우선 데이터셋 다운로드부터 먼저 시작한다. 데이터셋은 미리 업로드된 구글 드라이브 링크로부터 다운로드하게 된다. 업로드된 파일을 내려받기 전에 프로젝트 디렉터리에 `data` 폴더가 있는지 검사하고 그 폴더 안에 각 데이터의 `.zip` 파일이 미리 받아졌는지 먼저 확인하고 (md5 해싱 이용) 파일이 문제가 없으면 압축을 푼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "import hashlib\n",
    "import zipfile\n",
    "\n",
    "def get_hash(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "        hash = hashlib.md5(data).hexdigest()\n",
    "    \n",
    "    return hash\n",
    "\n",
    "data_info = {\n",
    "    \"cwru\": {\"link\": \"https://drive.google.com/uc?id=1JfnCzisg0wTSkWw_I5sNLcvQMD5mloJy\",\n",
    "             \"hash\": \"a66d9ea53e5b9959829c1d1057abc377\"},\n",
    "    \"mfpt\": {\"link\": \"https://drive.google.com/uc?id=1HDmX9-v8dV1-53nvM9lSDj-2-S2_Dss5\",\n",
    "             \"hash\": \"fcf44622538307e33503cb7f24fd33d3\"},\n",
    "    \"ottawa\": {\"link\": \"https://drive.google.com/uc?id=1WelJO5RMFwKoNdumhtW-__PC881fh4J_\",\n",
    "               \"hash\": \"ca0142f52e950b5579985586a6acc96a\"\n",
    "    }\n",
    "}\n",
    "\n",
    "os.chdir(os.path.join(globals()['_dh'][0], \"..\"))\n",
    "\n",
    "if not os.path.isdir(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "for key in data_info:\n",
    "    filename = f\"./data/{key}.zip\"\n",
    "    if not os.path.isfile(filename):\n",
    "        gdown.download(data_info[key][\"link\"], f\"./data/{key}.zip\")\n",
    "    else:\n",
    "        hash = get_hash(filename)\n",
    "        if hash != data_info[key][\"hash\"]:\n",
    "            os.remove(filename)\n",
    "            gdown.download(data_info[key][\"link\"], f\"./data/{key}.zip\")\n",
    "\n",
    "for key in data_info:\n",
    "    filename = f\"./data/{key}.zip\"\n",
    "    zipfile.ZipFile(filename).extractall(\"./data/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 빌딩 및 분할\n",
    "\n",
    "그 후, dfb 패키지에 사전 정의된 모듈을 활용하여 데이터프레임을 만든다. 현재 dfb의 download 모듈에서는 CWRU, MFPT, Paderborn, Ottawa 데이터셋을 자동으로 판다스 데이터프레임으로 변환해주는 기능을 제공한다. CWRU, MFPT, Paderborn 같은 경우는 리눅스 시스템에서 가동하는 기준으로 데이터 홈페이지에서 각 파일을 자동으로 다운로드해서 데이터프레임을 빌딩하는 기능도 제공하지만 이 예제에서는 편의성을 위해 구글드라이브 다운로드를 이용했다. Ottawa 데이터셋은 데이터 소스에서 바로 다운로드하는 기능을 제공하지 않으니 구글드라이브 다운로드를 이용해야 한다.\n",
    "\n",
    "### CWRU 데이터셋\n",
    "\n",
    "CWRU 데이터셋은 Case Western Reserve University가 제공하는 데이터셋이다 [링크](https://engineering.case.edu/bearingdatacenter/download-data-file). 이 데이터셋은 12, 24 kHz의 sampling rate, 1, 2, 3, hp의 load에서 수집된 진동 데이터를 제공한다. 각 데이터는 정상, 내륜/외륜/볼 결함의 4종류에서 측정된 진동 데이터로 구성되어 있다. 각 결함은 데이터 생성자들이 인공적으로 만들었으며 0.007, 0.014, 0.021, 0.028 inch 크기 베어링에 대한 데이터를 제공한다. 본 예제에서는 sampling rate 12 kHz, 결함 크기 0.007, 0.014, 0.021 inch 데이터를 사용한다. CWRU 데이터셋의 더 자세한 내용은 프로젝트 디렉터리의 `Optimizer Benchmark.pdf`의 Section III-A나 [이 논문](https://ieeexplore.ieee.org/document/9078761)을 참조하면 좋다.\n",
    "\n",
    "### MFPT 데이터셋\n",
    "\n",
    "MFPT 데이터셋은 Society for Machinery Failure Prevention Technology에서 제공하는 데이터셋이다 [링크](https://www.mfpt.org/fault-data-sets/). 이 데이터셋은 CWRU보다 조금 더 다양한 load condition에서 측정된 데이터를 제공한다. 데이터는 정상, 내/외륜 결함의 3종류에서 측정되었다. MFPT 데이터셋의 상세사항 역시 프로젝트 디렉터리의 `Optimizer Benchmark.pdf`의 Section III-A나 [이 논문](https://ieeexplore.ieee.org/document/9078761)을 참조하면 좋다.\n",
    "\n",
    "### Ottawa 데이터셋\n",
    "\n",
    "Ottawa 데이터셋은 동적으로 변하는 회전기계 환경에서 수집된 베어링 진동 데이터셋이다 [링크](https://data.mendeley.com/datasets/v43hmbwxpm/2). 이 데이터셋은 측정 중 모터 회전속도가 동적으로 변하는 상황에서 측정되었따. 데이터는 정상, 볼/내륜/외륜 결함의 4종류뿐만 아니라 모든 부분에 손상이 가해진 데이터도 제공한다. 다만 후술할 continual learning 시나리오에 맞추기 위해서 본 예제에서는 정상, 내륜/외륜 결함의 3가지 데이터셋만 사용했다. 이 데이터셋에 대한 상세한 내용은 데이터 제공자의 [논문](https://www.sciencedirect.com/science/article/pii/S2352340918314124?ref=cra_js_challenge&fr=RR-1)을 참고하면 좋다.\n",
    "\n",
    "### 데이터셋 분할\n",
    "\n",
    "**중요 변경점: 데이터를 훈련, 검증, 테스트로 분리하기 이전에 먼저 빌딩하면 test 데이터 내용이 train으로 섞일 수 있어서 분할 방법을 변경함.**\n",
    "\n",
    "데이터셋을 다운로드받은 후 각 데이터 세그먼트를 6:2:2 비율로 분할하여 train, validation, test 데이터셋으로 사용한다.\n",
    "\n",
    "### 각 데이터를 분석해보는 방법\n",
    "\n",
    "데이터를 분석하는 좋은 방법 중 하나는 수집된 진동 데이터를 `matplotlib`으로 시각화해보는 것이다. 다음의 구현을 통해 신호의 특성을 파악해볼 수 있다.\n",
    "\n",
    "* Raw signal을 plt.plot으로 시각화해보기. 한 데이터의 길이가 너무 길다면 일부를 잘라서 확인해보는 것도 좋음. 이 신호를 눈으로 확인해 보면 각 fault별로 어떤 특징들이 있는지 1차적으로 간단히 파악 가능함.\n",
    "* FFT 해보기: 각 신호의 주파수 성분 요소를 파악하기 위해 numpy같은 라이브러리에서 제공하는 FFT 함수를 통해 신호를 시각화해보면 신호가 가진 주요 주파수 대역을 알 수 있음. 조금 더 자세한 내용은 베어링 결함 진단 tutorial 논문인 FD_tutorial.pdf를 확인해보기\n",
    "* 각 신호의 통계량 계산해보기: 각 신호의 RMS, peak, Crest Factor, Kurtosis 등의 통계량을 계산해볼 수 있음. 자세한 내용은 저장소의 Vibration Analysis.pdf를 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is already existed, use existed file.\n"
     ]
    }
   ],
   "source": [
    "from dfb.download import *\n",
    "from dfb.databuilder import *\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "train_dfs = {}\n",
    "val_dfs = {}\n",
    "test_dfs = {}\n",
    "\n",
    "for key in data_info:\n",
    "    dfs[key] = download_data(f\"./data/{key}\", key)\n",
    "\n",
    "for key in data_info:\n",
    "    train_dfs[key], val_dfs[key], test_dfs[key] = split_dataframe(dfs[key], 0.6, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121991,)\n",
      "IR\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc29cb62b00>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_idx = 15\n",
    "\n",
    "x = dfs[\"cwru\"].iloc[data_idx][\"data\"]\n",
    "x_fft = np.fft.fft(x)\n",
    "freq = np.fft.fftfreq(x.shape[-1])\n",
    "idx_start = 6000\n",
    "\n",
    "print(x.shape)\n",
    "print(dfs[\"cwru\"].iloc[data_idx].fault_type)\n",
    "# plt.plot(x[idx_start:idx_start+4096])\n",
    "# plt.plot(x)\n",
    "\n",
    "plt.plot(freq[:x_fft.shape[0] // 2], np.abs(x_fft.real)[:x_fft.shape[0] // 2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Array 빌딩\n",
    "\n",
    "데이터프레임을 생성하고 나서, continual learning에 쓰일 데이터셋을 만들 것이다. continual learning에 사용될 데이터는 experience로 나뉜다. 모델은 각 experience에 해당하는 데이터를 순차적으로 학습하면서, 이전 experience에 대한 정확도가 떨어지지 않고 전체 데이터에 대해 높은 정확도를 기록하는 것을 목표로 한다.\n",
    "\n",
    "이 예제에서는 데이터프레임을 통해 level 1, 2, 3의 continual learning dataset을 만들 것이다. 데이터 level의 의미는 각 continual learning experience 간 데이터 도메인이 얼마나 크게 차이나는 지를 의미한다. 각 level은 다음과 같이 정의한다.\n",
    "\n",
    "* `level 1`: 각 experience는 모두 CWRU 데이터셋으로 사용된다. experience A, B, C는 각각 1, 2, 3 hp의 load 환경에서 수집된 fault 데이터이다. load가 다른 데이터의 클래스를 판별하는 태스크의 난이도는 1 hp의 데이터셋만으로 훈련을 진행하여 2, 3 hp의 데이터셋을 90% 이상으로 판별할 수 있을 정도로 쉽기 때문에 level 1로 두었다.\n",
    "\n",
    "* `level 2`: 각 experience는 모두 CWRU 데이터셋으로 사용된다. experience A, B, C는 각각 0.07, 0.14, 0.21 inch의 crack size에서 수집된 fault 데이터이다. 2022년 발표된 Hendricks et al.의 benchmark [연구](https://www.sciencedirect.com/science/article/pii/S0888327021010499)는 다른 load에서 수집된 데이터를 분류하는 태스크보다 다른 crack size에서 수집된 데이터를 분류하는 태스크의 정확도가 훨씬 낮다는 것을 보였다. 따라서 crack size가 다른 데이터가 추가로 들어오는 상황에서 학습을 진행하는 것을 level 2로 두었다. Healty bearing은 crack size가 없으므로 모든 experience에 일괄적으로 추가해 놓았다. 이 부분은 나중에 변경이 필요할 경우 변경하면 된다.\n",
    "\n",
    "* `level 3`: 각 experience A, B, C의 데이터셋이 CWRU, MFPT, Ottawa University로 모두 다르다. 이 세 데이터셋은 모두 회전 기계의 bearing fault class를 가진 진동 데이터셋이지만 측정 조건이 완전히 다르다. 따라서 dataset level이 다른 데이터가 추가로 들어오는 상황에서 학습을 진행하는 것을 level 3으로 두었다.\n",
    "\n",
    "데이터프레임은 databuilder 모듈의 `build_from_dataframe` 함수를 통해 numpy array로 변환된다. 이 함수에서 정하는 `sample_length`는 한 샘플의 데이터 길이를, `shift`는 데이터 세그멘트에서 샘플 데이터를 overlapping으로 추출할때의 shift size이다. 데이터를 추출하는 방법에 대한 자세한 설명은 내가 2022년에 쓴 논문 (프로젝트 디렉터리의 `Optimizer Benchmark.pdf`의 Section III-A의 Fig.3 참조)을 참조하면 된다.\n",
    "\n",
    "코드에서 CWRU 데이터셋 데이터프레임에 label이 999인 row를 제외시키는 코드가 있다. 현재 코드에서 CWRU 레이블이 999로 처리된 부분은 예전 논문에서 실험할 때 사용하지 않았던 데이터들이다. 나중에 코드를 수정해서 갖다 써도 무방하다.\n",
    "\n",
    "현재 구현되어 있는 모델의 input length의 최댓값이 4,096이라 `sample_length`는 기본적으로 4,096으로 두었다. 추후에 훈련을 진행할 때는 더 작은 길이의 샘플을 입력으로 받는 모델의 경우 샘플의 앞 부분만 잘라서 사용하면 된다.\n",
    "\n",
    "CWRU, MFPT 데이터셋은 데이터셋의 규모가 비슷하지만, Ottawa 데이터셋은 앞의 두 데이터셋보다 크기가 10배 이상 크다. experience의 규모가 크게 달라지는 경우를 방지하기 위해 `level 3` 데이터셋을 구축할 때는 각 세그먼트에서 정해진 `N`개의 샘플만 랜덤으로 복원추출하는 bootstrap 방식을 사용해서 클래스별 샘플 수를 다른 level과 비슷하도록 맞췄다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4033123/3446598653.py:107: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filter_cwru = filter_cwru[(df_cwru[\"fault_type\"] == \"N\") | (df_cwru[\"fault_type\"] == \"IR\") | (df_cwru[\"fault_type\"] == \"OR@06\")].reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "from dfb.databuilder import *\n",
    "\n",
    "train_df_cwru = train_dfs[\"cwru\"]\n",
    "train_df_mfpt = train_dfs[\"mfpt\"]\n",
    "train_df_ottawa = train_dfs[\"ottawa\"]\n",
    "\n",
    "val_df_cwru = val_dfs[\"cwru\"]\n",
    "val_df_mfpt = val_dfs[\"mfpt\"]\n",
    "val_df_ottawa = val_dfs[\"ottawa\"]\n",
    "\n",
    "test_df_cwru = test_dfs[\"cwru\"]\n",
    "test_df_mfpt = test_dfs[\"mfpt\"]\n",
    "test_df_ottawa = test_dfs[\"ottawa\"]\n",
    "\n",
    "sample_length = 4096\n",
    "shift = 2048\n",
    "\n",
    "# level 1 데이터셋 제작\n",
    "\n",
    "train_data_level1 = {}\n",
    "val_data_level1 = {}\n",
    "test_data_level1 = {}\n",
    "\n",
    "def generate_level1(df, sample_length, shift):\n",
    "    data = {}\n",
    "    df_A = df[(df[\"load\"] == 1) & (df[\"label\"] != 999)]\n",
    "    df_B = df[(df[\"load\"] == 2) & (df[\"label\"] != 999)]\n",
    "    df_C = df[(df[\"load\"] == 3) & (df[\"label\"] != 999)]\n",
    "\n",
    "    data[\"A\"] = build_from_dataframe(df_A, sample_length=sample_length, shift=shift)\n",
    "    data[\"B\"] = build_from_dataframe(df_B, sample_length=sample_length, shift=shift)\n",
    "    data[\"C\"] = build_from_dataframe(df_C, sample_length=sample_length, shift=shift)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data_level1 = generate_level1(train_df_cwru, sample_length, shift)\n",
    "val_data_level1 = generate_level1(val_df_cwru, sample_length, shift)\n",
    "test_data_level1 = generate_level1(test_df_cwru, sample_length, shift)\n",
    "\n",
    "# level 2 데이터셋 제작\n",
    "\n",
    "def set_label_level2(row):\n",
    "    label_map = {\n",
    "        \"N\": 0,\n",
    "        \"B\": 1,\n",
    "        \"IR\": 2,\n",
    "        \"OR@06\": 3\n",
    "    }\n",
    "    row[\"label\"] = label_map[row[\"fault_type\"]]\n",
    "    return row\n",
    "\n",
    "def generate_level2(df, sample_length, shift):\n",
    "    data = {}\n",
    "    df_normal = df[(df[\"fault_type\"] == \"N\")]\n",
    "    df_007 = df[(df[\"crack_size\"] == \"007\") & (df[\"label\"] != 999)]\n",
    "    df_014 = df[(df[\"crack_size\"] == \"014\") & (df[\"label\"] != 999)]\n",
    "    df_021 = df[(df[\"crack_size\"] == \"021\") & (df[\"label\"] != 999)]\n",
    "    df_A = pd.concat(objs=(df_normal, df_007)).apply(set_label_level2, axis=\"columns\")\n",
    "    df_B = pd.concat(objs=(df_normal, df_014)).apply(set_label_level2, axis=\"columns\")\n",
    "    df_C = pd.concat(objs=(df_normal, df_021)).apply(set_label_level2, axis=\"columns\")\n",
    "\n",
    "    data[\"A\"] = build_from_dataframe(df_A, sample_length=sample_length, shift=shift)\n",
    "    data[\"B\"] = build_from_dataframe(df_B, sample_length=sample_length, shift=shift)\n",
    "    data[\"C\"] = build_from_dataframe(df_C, sample_length=sample_length, shift=shift)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data_level2 = generate_level2(train_df_cwru, sample_length, shift)\n",
    "val_data_level2 = generate_level2(val_df_cwru, sample_length, shift)\n",
    "test_data_level2 = generate_level2(test_df_cwru, sample_length, shift)\n",
    "\n",
    "# level 3 데이터셋 제작\n",
    "\n",
    "data_level3 = {}\n",
    "\n",
    "def set_label_level3(row):\n",
    "    label_map = {\n",
    "        \"N\": 0,\n",
    "        \"IR\": 1,\n",
    "        \"OR@06\": 2,\n",
    "        \"OR\": 2\n",
    "    }\n",
    "    row[\"label\"] = label_map[row[\"fault_type\"]]\n",
    "    return row\n",
    "\n",
    "def generate_level3(df_cwru, df_mfpt, df_ottawa, sample_length, sample_ratio):\n",
    "    sample_map = {\n",
    "        \"cwru\": {\n",
    "            \"0\": round(1*105*sample_ratio),\n",
    "            \"1\": round(1*35*sample_ratio),\n",
    "            \"2\": round(1*35*sample_ratio)\n",
    "        },\n",
    "        \"mfpt\": {\n",
    "            \"0\": round(1*140*sample_ratio),\n",
    "            \"1\": round(1*60*sample_ratio),\n",
    "            \"2\": round(1*42*sample_ratio)\n",
    "        },\n",
    "        \"ottawa\": {\n",
    "            \"0\": round(1*35*sample_ratio),\n",
    "            \"1\": round(1*35*sample_ratio),\n",
    "            \"2\": round(1*35*sample_ratio)\n",
    "        }\n",
    "    }\n",
    "    data = {}\n",
    "\n",
    "    filter_cwru = df_cwru[df_cwru[\"label\"] != 999]\n",
    "    filter_cwru = filter_cwru[(df_cwru[\"fault_type\"] == \"N\") | (df_cwru[\"fault_type\"] == \"IR\") | (df_cwru[\"fault_type\"] == \"OR@06\")].reset_index(drop=True)\n",
    "    filter_mfpt = df_mfpt[(df_mfpt[\"fault_type\"] == \"N\") | (df_mfpt[\"fault_type\"] == \"IR\") | (df_mfpt[\"fault_type\"] == \"OR\")].reset_index(drop=True)\n",
    "    filter_ottawa = df_ottawa[(df_ottawa[\"fault_type\"] == \"N\") | (df_ottawa[\"fault_type\"] == \"IR\") | (df_ottawa[\"fault_type\"] == \"OR\")].reset_index(drop=True)\n",
    "\n",
    "    filter_cwru = filter_cwru.apply(set_label_level3, axis=\"columns\")\n",
    "    filter_mfpt = filter_mfpt.apply(set_label_level3, axis=\"columns\")\n",
    "    filter_ottawa = filter_ottawa.apply(set_label_level3, axis=\"columns\")\n",
    "\n",
    "    data[\"A\"] = bootstrap_from_dataframe(filter_cwru, sample_length=sample_length, n_sample=100, one_hot=False, n_map=sample_map[\"cwru\"])\n",
    "    data[\"B\"] = bootstrap_from_dataframe(filter_mfpt, sample_length=sample_length, n_sample=100, one_hot=False, n_map=sample_map[\"mfpt\"])\n",
    "    data[\"C\"] = bootstrap_from_dataframe(filter_ottawa, sample_length=sample_length, n_sample=100, one_hot=False, n_map=sample_map[\"ottawa\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data_level3 = generate_level3(train_df_cwru, train_df_mfpt, train_df_ottawa, sample_length, 0.6)\n",
    "val_data_level3 = generate_level3(val_df_cwru, val_df_mfpt, val_df_ottawa, sample_length, 0.2)\n",
    "test_data_level3 = generate_level3(test_df_cwru, test_df_mfpt, test_df_ottawa, sample_length, 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 합치기\n",
    "\n",
    "이 튜토리얼에서는 Continual learning의 upper bound인 joint training, 즉 모든 experience의 데이터를 한번에 학습하는 케이스를 다룬다. 이를 통해 기본적인 모델의 성능을 평가하고, 그 후 continual learning 시나리오에 적용하는 스텝을 밟으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Level 1\n",
    "X_train_1 = np.vstack(\n",
    "    [train_data_level1[\"A\"][0],\n",
    "     train_data_level1[\"B\"][0],\n",
    "     train_data_level1[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_train_1 = np.hstack(\n",
    "    [train_data_level1[\"A\"][1],\n",
    "     train_data_level1[\"B\"][1],\n",
    "     train_data_level1[\"C\"][1]]\n",
    ")\n",
    "\n",
    "X_val_1 = np.vstack(\n",
    "    [val_data_level1[\"A\"][0],\n",
    "     val_data_level1[\"B\"][0],\n",
    "     val_data_level1[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_val_1 = np.hstack(\n",
    "    [val_data_level1[\"A\"][1],\n",
    "     val_data_level1[\"B\"][1],\n",
    "     val_data_level1[\"C\"][1]]\n",
    ")\n",
    "\n",
    "X_test_1 = np.vstack(\n",
    "    [test_data_level1[\"A\"][0],\n",
    "     test_data_level1[\"B\"][0],\n",
    "     test_data_level1[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_test_1 = np.hstack(\n",
    "    [test_data_level1[\"A\"][1],\n",
    "     test_data_level1[\"B\"][1],\n",
    "     test_data_level1[\"C\"][1]]\n",
    ")\n",
    "\n",
    "# ====================== Level 2\n",
    "X_train_2 = np.vstack(\n",
    "    [train_data_level2[\"A\"][0],\n",
    "     train_data_level2[\"B\"][0],\n",
    "     train_data_level2[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_train_2 = np.hstack(\n",
    "    [train_data_level2[\"A\"][1],\n",
    "     train_data_level2[\"B\"][1],\n",
    "     train_data_level2[\"C\"][1]]\n",
    ")\n",
    "\n",
    "X_val_2 = np.vstack(\n",
    "    [val_data_level2[\"A\"][0],\n",
    "     val_data_level2[\"B\"][0],\n",
    "     val_data_level2[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_val_2 = np.hstack(\n",
    "    [val_data_level2[\"A\"][1],\n",
    "     val_data_level2[\"B\"][1],\n",
    "     val_data_level2[\"C\"][1]]\n",
    ")\n",
    "\n",
    "X_test_2 = np.vstack(\n",
    "    [test_data_level2[\"A\"][0],\n",
    "     test_data_level2[\"B\"][0],\n",
    "     test_data_level2[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_test_2 = np.hstack(\n",
    "    [test_data_level2[\"A\"][1],\n",
    "     test_data_level2[\"B\"][1],\n",
    "     test_data_level2[\"C\"][1]]\n",
    ")\n",
    "\n",
    "# ====================== Level 3\n",
    "X_train_3 = np.vstack(\n",
    "    [train_data_level3[\"A\"][0],\n",
    "     train_data_level3[\"B\"][0],\n",
    "     train_data_level3[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_train_3 = np.hstack(\n",
    "    [train_data_level3[\"A\"][1],\n",
    "     train_data_level3[\"B\"][1],\n",
    "     train_data_level3[\"C\"][1]]\n",
    ")\n",
    "\n",
    "X_val_3 = np.vstack(\n",
    "    [val_data_level3[\"A\"][0],\n",
    "     val_data_level3[\"B\"][0],\n",
    "     val_data_level3[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_val_3 = np.hstack(\n",
    "    [val_data_level3[\"A\"][1],\n",
    "     val_data_level3[\"B\"][1],\n",
    "     val_data_level3[\"C\"][1]]\n",
    ")\n",
    "\n",
    "X_test_3 = np.vstack(\n",
    "    [test_data_level3[\"A\"][0],\n",
    "     test_data_level3[\"B\"][0],\n",
    "     test_data_level3[\"C\"][0]]\n",
    ")\n",
    "\n",
    "y_test_3 = np.hstack(\n",
    "    [test_data_level3[\"A\"][1],\n",
    "     test_data_level3[\"B\"][1],\n",
    "     test_data_level3[\"C\"][1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVEL 1 ===\n",
      "(1341, 4096) (1341,)\n",
      "(409, 4096) (409,)\n",
      "(409, 4096) (409,)\n",
      "LEVEL 2 ===\n",
      "(2701, 4096) (2701,)\n",
      "(841, 4096) (841,)\n",
      "(841, 4096) (841,)\n",
      "LEVEL 3 ===\n",
      "(2266, 4096) (2266,)\n",
      "(752, 4096) (752,)\n",
      "(752, 4096) (752,)\n"
     ]
    }
   ],
   "source": [
    "print(\"LEVEL 1 ===\")\n",
    "print(X_train_1.shape, y_train_1.shape)\n",
    "print(X_val_1.shape, y_val_1.shape)\n",
    "print(X_test_1.shape, y_test_1.shape)\n",
    "\n",
    "print(\"LEVEL 2 ===\")\n",
    "print(X_train_2.shape, y_train_2.shape)\n",
    "print(X_val_2.shape, y_val_2.shape)\n",
    "print(X_test_2.shape, y_test_2.shape)\n",
    "\n",
    "print(\"LEVEL 3 ===\")\n",
    "print(X_train_3.shape, y_train_3.shape)\n",
    "print(X_val_3.shape, y_val_3.shape)\n",
    "print(X_test_3.shape, y_test_3.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터로더 래핑\n",
    "\n",
    "numpy array를 PyTorch에서 사용가능한 데이터로더로 만들기 위해서는 추가로 모델 입력의 길이, 모델의 transform (데이터 전처리 함수), 배치 크기, num_worker가 필요하다. 모델 입력의 길이와 transform은 모델에 따라 달라진다. 현재 이 예제에서 구현되어 있는 모델의 입력 길이와 transform은 experiment 모듈의 get_sample_length와 get_transform을 통해 바로 가져올 수 있다. 그렇게 한 후에 dataset 모듈에서 제공하는 `DatasetHandler`클래스를 사용해서 각 level과 experience에 해당하는 numpy array를 `assign` 메소드를 통해 PyTorch 데이터로더로 변환하게 된다.\n",
    "\n",
    "각 데이터는 `DatasetHandler`의 `dataloaders`멤버의 key에 접근하여 확인할 수 있고, 데이터로더는 `dataset`이라는 멤버를 가져서 그 멤버 안에서 개별 데이터를 확인할 수 있다.\n",
    "\n",
    "이 부분을 이해하기 위해서는 다음과 같은 내용을 찾아보면 좋다.\n",
    "* PyTorch는 훈련 데이터셋을 어떻게 다루는지? 이미지, numpy, 또는 Tensor 데이터에서 어떤 과정으로 `DataLoader`를 만드는지 PyTorch 공식 문서에서 확인해 볼것\n",
    "* PyTorch는 개별 데이터를 transforms라는 모듈을 통해 전처리함. Raw 데이터는 모델에 훈련 입력으로 들어가기 전에 transforms의 규칙에 따라 전처리되게 됨. 이 부분을 PyTorch 공식 문서에서 확인해 볼 것.\n",
    "* 미리 정의된 모델 및 transforms는 experiment.py 파일의 model_info 딕셔너리에 들어가 있음. 이걸 참고해서 어떤 방식으로 데이터가 전처리되는지 확인해 볼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/happy113200/anaconda3/envs/cfb/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dfb.dataset import *\n",
    "from dfb.processing import *\n",
    "import experiment\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "model_name = \"wdcnn\"\n",
    "\n",
    "sample_length = experiment.get_sample_length(model_name)\n",
    "tf_data = experiment.get_transform(model_name)\n",
    "tf_data = transforms.Compose(tf_data)\n",
    "tf_label = NpToTensor()\n",
    "batch_size = 128\n",
    "num_worker = 4\n",
    "\n",
    "data_handler = DatasetHandler()\n",
    "\n",
    "# ============================== level 1\n",
    "\n",
    "data_handler.assign(\n",
    "    X_train_1, y_train_1, X_val_1, y_val_1, X_test_1, y_test_1,\n",
    "    sample_length, \"1\", tf_data, tf_label, batch_size, num_worker\n",
    ")\n",
    "\n",
    "data_handler.assign(\n",
    "    X_train_2, y_train_2, X_val_2, y_val_2, X_test_2, y_test_2,\n",
    "    sample_length, \"2\", tf_data, tf_label, batch_size, num_worker\n",
    ")\n",
    "\n",
    "data_handler.assign(\n",
    "    X_train_3, y_train_3, X_val_3, y_val_3, X_test_3, y_test_3,\n",
    "    sample_length, \"3\", tf_data, tf_label, batch_size, num_worker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0461, -0.0371, -0.0895,  ...,  0.0013, -0.0271, -0.0098]]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터로더 접근하기\n",
    "data_handler.dataloaders[\"1\"][\"train\"]\n",
    "\n",
    "#데이터셋 접근하기\n",
    "data_handler.dataloaders[\"1\"][\"train\"].dataset\n",
    "\n",
    "#원본 numpy array 접근하기\n",
    "data_handler.dataloaders[\"1\"][\"train\"].dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 가져오기\n",
    "\n",
    "모델은 아주 간단하게 가져올 수 있다. dfb의 model 폴더에 있는 모델을 아무거나 가져오면 된다. 해당 폴더 안에는 지금까지 구현했던 모델이 모두 들어있으며, 해당 모델에 대한 transforms와 sample_length는 위에서 설명했던 것과 같이 experiment.py 폴더의 model_info 딕셔너리에서 모두 얻을 수 있다. 이 단계에서 추후 확장성을 위해서는 PyTorch의 모델을 구현하는 능력이 있어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WDCNN 가져오기\n",
    "\n",
    "from dfb.model.wdcnn import *\n",
    "# 클래스 수는 맘대로 지정\n",
    "\n",
    "# 클래스 수 10개짜리 모델 만들기\n",
    "model = WDCNN(n_classes=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련하기\n",
    "\n",
    "모델을 훈련하기 위해서는 다음과 같은 요소들이 필요하다.\n",
    "\n",
    "* 데이터로더\n",
    "* 모델\n",
    "* 최적화 알고리즘\n",
    "* 손실함수\n",
    "* 훈련을 위한 정보: GPU 번호, 에포크\n",
    "\n",
    "지금까지의 과정에서 데이터로더와 모델을 완성했다. 그리고 torch에서 제공하는 optimizer와 loss를 사용하면 되고, GPU 번호와 에포크는 사용자가 원하는 만큼 지정하면 된다. 이 튜토리얼에서의 훈련 과정에서는 PyTorch Lightning 라이브러리를 사용하여 훈련을 수행한다. PyTorch Lightning은 PyTorch의 high level 래퍼로, 훈련 코드를 간단하고 추상화된 형태로 만들어 준다. 이 튜토리얼에서는 직접 구현한 `PlModule` 클래스를 활용하여 훈련을 수행한다. 타고 들어가보면 PyTorch lightning에서 요구하는 형태의 순전파, 훈련스텝, 검증스텝, 로깅 등이 구현되어 있다. 이 코드를 이해하기 위해서는 PyTorch lightning의 코드에 대한 이해도가 있어야 되서 추가적인 공부가 필요하다.\n",
    "\n",
    "추천하는 방법은 아래 마무리 부분에서 설명할 것처럼 이 튜토리얼의 모든 내용을 직접 만들어 보는 것이다. 맨 앞부분에서 numpy 데이터를 가져오는 부분까지만 사용한 후 그 다음 부분을 직접 구현해보면 살짝 감이 올 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/happy113200/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/data/home/happy113200/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /data/home/happy113200/DeepFault/logs/test/best_model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | model   | WDCNN            | 113 K \n",
      "1 | loss_fn | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "113 K     Trainable params\n",
      "0         Non-trainable params\n",
      "113 K     Total params\n",
      "0.454     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/happy113200/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  96%|█████████▌| 23/24 [00:00<00:00, 26.73it/s, loss=0.901, v_num=18]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 05:20:59.556565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-14 05:20:59.724502: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-14 05:21:00.413943: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.2/lib64\n",
      "2023-07-14 05:21:00.414036: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.2/lib64\n",
      "2023-07-14 05:21:00.414042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 24/24 [00:00<00:00, 26.41it/s, loss=0.271, v_num=18, val_loss=0.271, val_acc=0.854, train_loss=0.261, train_acc=0.878]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 24/24 [00:00<00:00, 26.28it/s, loss=0.271, v_num=18, val_loss=0.271, val_acc=0.854, train_loss=0.261, train_acc=0.878]\n"
     ]
    }
   ],
   "source": [
    "from dfb.trainmodule import *\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# ticnn 플래그는 일단 신경쓸 필요 X\n",
    "training_module = PlModule(model, optimizer, loss, True)\n",
    "\n",
    "n_steps_d = len(data_handler.dataloaders[\"3\"][\"train\"].dataset) // (batch_size)\n",
    "\n",
    "# val_loss 가 가장 작은 모델을 저장하는 콜백\n",
    "callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                        dirpath=f\"./logs/test/best_model\",\n",
    "                                        filename=f\"model\",\n",
    "                                        save_top_k=1,\n",
    "                                        mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    gpus = [0],\n",
    "    max_epochs=10,\n",
    "    val_check_interval= n_steps_d,\n",
    "    default_root_dir=\"./logs/test\",\n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "result = trainer.fit(model=training_module,\n",
    "                     train_dataloaders=data_handler.dataloaders[\"3\"][\"train\"],\n",
    "                     val_dataloaders=data_handler.dataloaders[\"3\"][\"val\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트하기\n",
    "\n",
    "훈련이 끝나면 val_loss가 가장 낮은 모델을 불러와서 테스트 데이터셋에 대해 테스트하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PlModule:\n\tUnexpected key(s) in state_dict: \"model.conv_layers.12.weight\", \"model.conv_layers.12.bias\", \"model.conv_layers.13.weight\", \"model.conv_layers.13.bias\", \"model.conv_layers.13.running_mean\", \"model.conv_layers.13.running_var\", \"model.conv_layers.13.num_batches_tracked\", \"model.conv_layers.16.weight\", \"model.conv_layers.16.bias\", \"model.conv_layers.17.weight\", \"model.conv_layers.17.bias\", \"model.conv_layers.17.running_mean\", \"model.conv_layers.17.running_var\", \"model.conv_layers.17.num_batches_tracked\". \n\tsize mismatch for model.conv_layers.4.weight: copying a param with shape torch.Size([32, 16, 3]) from checkpoint, the shape in current model is torch.Size([64, 16, 3]).\n\tsize mismatch for model.conv_layers.4.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.5.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.5.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.5.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.5.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.8.weight: copying a param with shape torch.Size([64, 32, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3]).\n\tsize mismatch for model.linear_layers.0.weight: copying a param with shape torch.Size([100, 192]) from checkpoint, the shape in current model is torch.Size([100, 960]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_module\u001b[39m.\u001b[39;49mload_from_checkpoint(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./logs/test/best_model/model.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m                                             model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m      3\u001b[0m                                             loss_fn\u001b[39m=\u001b[39;49mloss)\n\u001b[1;32m      4\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest(model\u001b[39m=\u001b[39mtraining_module, dataloaders\u001b[39m=\u001b[39mdata_handler\u001b[39m.\u001b[39mdataloaders[\u001b[39m\"\u001b[39m\u001b[39m3\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:137\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_from_checkpoint(\n\u001b[1;32m    138\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m    139\u001b[0m         checkpoint_path,\n\u001b[1;32m    140\u001b[0m         map_location,\n\u001b[1;32m    141\u001b[0m         hparams_file,\n\u001b[1;32m    142\u001b[0m         strict,\n\u001b[1;32m    143\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:205\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39m, checkpoint, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 205\u001b[0m \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39;49m, checkpoint, strict\u001b[39m=\u001b[39;49mstrict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:259\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n\u001b[1;32m    258\u001b[0m \u001b[39m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m keys \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m\"\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m\"\u001b[39;49m], strict\u001b[39m=\u001b[39;49mstrict)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m strict:\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m keys\u001b[39m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/anaconda3/envs/cfb/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PlModule:\n\tUnexpected key(s) in state_dict: \"model.conv_layers.12.weight\", \"model.conv_layers.12.bias\", \"model.conv_layers.13.weight\", \"model.conv_layers.13.bias\", \"model.conv_layers.13.running_mean\", \"model.conv_layers.13.running_var\", \"model.conv_layers.13.num_batches_tracked\", \"model.conv_layers.16.weight\", \"model.conv_layers.16.bias\", \"model.conv_layers.17.weight\", \"model.conv_layers.17.bias\", \"model.conv_layers.17.running_mean\", \"model.conv_layers.17.running_var\", \"model.conv_layers.17.num_batches_tracked\". \n\tsize mismatch for model.conv_layers.4.weight: copying a param with shape torch.Size([32, 16, 3]) from checkpoint, the shape in current model is torch.Size([64, 16, 3]).\n\tsize mismatch for model.conv_layers.4.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.5.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.5.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.5.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.5.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for model.conv_layers.8.weight: copying a param with shape torch.Size([64, 32, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3]).\n\tsize mismatch for model.linear_layers.0.weight: copying a param with shape torch.Size([100, 192]) from checkpoint, the shape in current model is torch.Size([100, 960])."
     ]
    }
   ],
   "source": [
    "training_module.load_from_checkpoint(f\"./logs/test/best_model/model.ckpt\",\n",
    "                                            model=model, optimizer=optimizer,\n",
    "                                            loss_fn=loss)\n",
    "result = trainer.test(model=training_module, dataloaders=data_handler.dataloaders[\"3\"][\"test\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이제부터 해야될 것\n",
    "\n",
    "이 튜토리얼은 베어링 결함 진단 과정이 어떻게 진행되는지 보여주고자 제작된 것이고, 대부분의 구현 및 훈련과정은 미리 모듈 형태로 구현된 소스코드를 사용한다. 따라서 이해를 높이기 위해 다음 스텝을 밟아서 추가적인 공부를 진행해봐야 한다.\n",
    "\n",
    "\n",
    "* PyTorch 기반 훈련 프로세스 만들어보기: `PlMoudle`이 해주고 있는 훈련 프로세스를 PyTorch로 그대로 구현해 보자. 우선 PyTorch의 훈련 프로세스를 이해한 후에 PyTorch lightning으로 구현해보는 것을 추천한다.\n",
    "* 모델 바꿔보기: 일단 현재 코드에서 모델을 다른 걸로 바꿔 본다. STFTCNN이나 STIMCNN은 2D CNN 계열의 모델로 전처리가 좀 더 복잡하다. 자세한 모델의 구조는 Optimizer Benchmark 논문을 확인해보자. 여러 모델을 돌려가면서 transforms에서 어떤 전처리가 일어나는지, 각 모델은 어떤 식으로 구현되어 있는지 공부를 해보는게 우선이다\n",
    "* 모델 직접 구현해보기: 아주 간단한 1D나 2D CNN이라도 괜찮고 이미 있는 image classification 모델을 가져와도 괜찮다. 자유로운 방식으로 custom 모델과 transforms를 구현한 뒤에 검증해보는 것이 다음 단계이다.\n",
    "* 데이터로더 만들기: PyTorch 공식 문서에서 `Dataset`, `DataLoader`를 공부한 뒤 직접 데이터 가져오는 부분을 구현해 보자. numpy array 형 데이터를 가져와 PyTorch에서 자유롭게 훈련할 수 있는 이해도가 되었다면 충분하다.\n",
    "* 데이터 뜯어보기: 마지막 단계이다. 응용에 대한 이해도를 높이기 위해 위에서 잠깐 설명했던 것처럼 데이터를 시각화하거나 신호처리 해보면서 데이터를 뜯어보는게 목표이다.\n",
    "\n",
    "위의 단계를 전부 수행하면 기본적으로 베어링 결함진단 코드를 작성하고 관련 논문의 구현요소를 이해하는 데는 어려움이 없어질 것으로 보인다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "804fb28fd32f99d3716759da6975404606d256093f726efcd3705b83b03ed366"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
